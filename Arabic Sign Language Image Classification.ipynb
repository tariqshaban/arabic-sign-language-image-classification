{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tariqshaban/arabic-sign-language-image-classification/blob/master/Arabic%20Sign%20Language%20Image%20Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Dependencies"
      ],
      "metadata": {
        "id": "jJI--RFbZOnp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display generic output messages\n",
        "!pip install colorama\n",
        "\n",
        "# Properly display Arabic characters in plots\n",
        "!pip install arabic_reshaper\n",
        "!pip install python-bidi\n",
        "\n",
        "# Download assets from the GitHub repository\n",
        "!apt install subversion\n",
        "!svn checkout https://github.com/tariqshaban/arabic-sign-language-image-classification/trunk/assets\n",
        "\n",
        "import arabic_reshaper\n",
        "import os\n",
        "import re\n",
        "import random\n",
        "import shutil\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "\n",
        "from bidi.algorithm import get_display\n",
        "from colorama import Fore, Style\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.layers import Dense, Dropout, GlobalAveragePooling2D, Layer\n",
        "from keras.models import Sequential\n",
        "from keras_preprocessing.image import ImageDataGenerator\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from tensorflow.keras.applications.efficientnet import EfficientNetB0\n",
        "from glob import glob\n",
        "from IPython.display import clear_output\n",
        "\n",
        "clear_output()\n",
        "print(Fore.GREEN + u'\\u2713 ' + 'Successfully downloaded dependencies.')    \n",
        "print(Style.RESET_ALL)"
      ],
      "metadata": {
        "id": "yEMk5leUZRyh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Global Variables"
      ],
      "metadata": {
        "id": "_Dw_FNXBeewu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CLASSES_DF = pd.read_excel('./assets/Labels/ClassLabels.xlsx', index_col='ClassId')\n",
        "CLASSES_DF.sort_values('ClassAr', inplace=True)\n",
        "\n",
        "CLASSES = CLASSES_DF['Class'].to_list()\n",
        "CLASSES_AR = CLASSES_DF['ClassAr'].to_list()\n",
        "\n",
        "SEED = 42\n",
        "\n",
        "TRAIN_SPLIT = 0.7\n",
        "VALID_SPLIT = 0.2\n",
        "TEST_SPLIT = 0.1\n",
        "\n",
        "SOURCE_DIRECTORY = './assets/ArASL_Database_54K/'\n",
        "REFACTORED_DIRECTORY = './assets/refactored_data/'\n",
        "TRAIN_DIRECTORY = f'{REFACTORED_DIRECTORY}train/'\n",
        "VALID_DIRECTORY = f'{REFACTORED_DIRECTORY}valid/'\n",
        "TEST_DIRECTORY = f'{REFACTORED_DIRECTORY}test/'\n",
        "\n",
        "EPOCHS = 2\n",
        "LEARNING_RATE = 0.001\n",
        "\n",
        "BASE_MODEL = EfficientNetB0(weights='imagenet', include_top=False)\n",
        "\n",
        "PREPROCESSING_METHOD = tf.keras.applications.efficientnet.preprocess_input"
      ],
      "metadata": {
        "id": "OV88E8Ssf_3E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helper Methods"
      ],
      "metadata": {
        "id": "rUbND8r93Av0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Finalize Seed"
      ],
      "metadata": {
        "id": "-bvirFR0UR-y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def finalize_seed():\n",
        "    os.environ['PYTHONHASHSEED'] = str(SEED)\n",
        "    random.seed(SEED)\n",
        "    np.random.seed(SEED)\n",
        "    tf.random.set_seed(SEED)"
      ],
      "metadata": {
        "id": "vNiHnAvuUSLn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prime Dataset"
      ],
      "metadata": {
        "id": "9SqhCp8E4ItZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prime_dataset():\n",
        "    if os.path.exists(REFACTORED_DIRECTORY):\n",
        "        shutil.rmtree(REFACTORED_DIRECTORY)\n",
        "\n",
        "    # Create Training, Validation, and Testing directories\n",
        "    for c in CLASSES:\n",
        "        os.makedirs(f'{TRAIN_DIRECTORY}{c}', exist_ok=True)\n",
        "        os.makedirs(f'{VALID_DIRECTORY}{c}', exist_ok=True)\n",
        "        os.makedirs(f'{TEST_DIRECTORY}{c}', exist_ok=True)\n",
        "\n",
        "    # Partition Images into Training, Validation, and Testing\n",
        "    for c in CLASSES:\n",
        "        numOfFiles = len(next(os.walk(f'{SOURCE_DIRECTORY}{c}/'))[2])\n",
        "        \n",
        "        for files in random.sample(glob(f'{SOURCE_DIRECTORY}{c}/*'), int(numOfFiles * TRAIN_SPLIT)):\n",
        "            shutil.move(files, f'{TRAIN_DIRECTORY}{c}')\n",
        "\n",
        "        for files in random.sample(glob(f'{SOURCE_DIRECTORY}{c}/*'), int(numOfFiles * VALID_SPLIT)):\n",
        "            shutil.move(files, f'{VALID_DIRECTORY}{c}')\n",
        "\n",
        "        for files in glob(f'{SOURCE_DIRECTORY}{c}/*'):\n",
        "            shutil.move(files, f'{TEST_DIRECTORY}{c}')"
      ],
      "metadata": {
        "id": "RM21XJfKg2PK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build Model"
      ],
      "metadata": {
        "id": "7e0duXPE4MhG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(measure_performance: bool = True):\n",
        "    train_batches = ImageDataGenerator(preprocessing_function=PREPROCESSING_METHOD).flow_from_directory(\n",
        "        directory=TRAIN_DIRECTORY, classes=CLASSES, batch_size=128)\n",
        "    valid_batches = ImageDataGenerator(preprocessing_function=PREPROCESSING_METHOD).flow_from_directory(\n",
        "        directory=VALID_DIRECTORY, classes=CLASSES, batch_size=128, shuffle=False)\n",
        "    test_batches = ImageDataGenerator(preprocessing_function=PREPROCESSING_METHOD).flow_from_directory(\n",
        "        directory=TEST_DIRECTORY, classes=CLASSES, batch_size=128, shuffle=False)\n",
        "\n",
        "    nclass = len(CLASSES)\n",
        "    epoch = EPOCHS\n",
        "    base_model = BASE_MODEL\n",
        "    base_model.trainable = False\n",
        "\n",
        "    add_model = Sequential()\n",
        "    add_model.add(base_model)\n",
        "    add_model.add(GlobalAveragePooling2D())\n",
        "    add_model.add(Dropout(0.5))\n",
        "    add_model.add(Dense(nclass, activation='softmax'))\n",
        "\n",
        "    model = add_model\n",
        "    model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=LEARNING_RATE), loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    es = EarlyStopping(monitor='val_loss', mode='auto', verbose=1, patience=10)\n",
        "\n",
        "    fitted_model = model.fit(x=train_batches, validation_data=valid_batches, epochs=epoch, callbacks=[es])\n",
        "    score, accuracy = model.evaluate(x=test_batches, batch_size=128)\n",
        "\n",
        "    print(Fore.GREEN + u'\\n\\u2713 ' + f'Accuracy ==> {accuracy}')\n",
        "\n",
        "    plt.rcParams[\"figure.figsize\"] = (15, 8)\n",
        "\n",
        "    if measure_performance:\n",
        "        plt.plot(fitted_model.history['accuracy'])\n",
        "        plt.plot(fitted_model.history['val_accuracy'])\n",
        "        plt.title('Model accuracy')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.legend(['Train', 'Test'], loc='upper left')\n",
        "        plt.show()\n",
        "\n",
        "        plt.plot(fitted_model.history['loss'])\n",
        "        plt.plot(fitted_model.history['val_loss'])\n",
        "        plt.title('Model loss')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.legend(['Train', 'Test'], loc='upper left')\n",
        "        plt.show()\n",
        "\n",
        "        y_pred = model.predict(test_batches)\n",
        "\n",
        "        labels = [f'{ar} ({en})' for ar, en in zip(CLASSES_AR, CLASSES)]\n",
        "        labels = [get_display(arabic_reshaper.reshape(label)) for label in labels]\n",
        "\n",
        "        ax = sns.heatmap(confusion_matrix(test_batches.classes, y_pred.argmax(axis=1)), annot=True, cmap='Blues',\n",
        "                         fmt='g')\n",
        "        ax.set_title('Confusion Matrix')\n",
        "        ax.set_xlabel('Predicted Values')\n",
        "        ax.set_ylabel('Actual Values')\n",
        "        ax.xaxis.set_ticklabels(labels)\n",
        "        ax.yaxis.set_ticklabels(labels)\n",
        "        plt.xticks(rotation=90)\n",
        "        plt.yticks(rotation=0)\n",
        "        plt.show()\n",
        "\n",
        "    return fitted_model"
      ],
      "metadata": {
        "id": "ZW9-46V-mttx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Methods Invocation"
      ],
      "metadata": {
        "id": "0e1-pG9A4SrD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "finalize_seed()"
      ],
      "metadata": {
        "id": "0RCW8uYPUH_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prime_dataset()"
      ],
      "metadata": {
        "id": "SVrT78Kvz05u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model(measure_performance=True)"
      ],
      "metadata": {
        "id": "agrTSMsfy5Fx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}